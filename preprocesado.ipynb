import numpy as np
df_missing = df.copy()
mask = np.random.rand(*df_missing.iloc[:, :-1].shape) < 0.05  
df_missing.iloc[:, :-1] = df_missing.iloc[:, :-1].mask(mask)

df_missing.describe()
df_missing.isnull().sum()

df2 = df.iloc[:-2754].reset_index(drop=True)
df2.describe()

from sklearn.preprocessing import LabelEncoder as Le
for col in df2.columns:
    encoder = Le()
    df2[col] = encoder.fit_transform(df2[col])

scaler = StandardScaler()
X_standard = scaler.fit_transform(df2.iloc[:, :-1])  # Only scale features, not target

df_X = pd.DataFrame(X_standard, columns=df2.columns[:-1], index=df2.index)

df_X.describe()

y = df2['target']

X_train, X_test, y_train, y_test = train_test_split(X_standard, y, stratify=y, test_size=0.2, random_state=42)
print(df2.head())

# Hiperparametros para el modelo de Random Forest

from sklearn.model_selection import GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [2, 4, 8],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

rf = RandomForestClassifier(random_state=42)
grid = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)
grid.fit(X_train, y_train)

print("Mejores hiperparámetros:", grid.best_params_)
print("Mejor score de validación:", grid.best_score_)

# Mejores Hiperparametros para Regresion Logistica

log_reg = LogisticRegression(max_iter=1000)
param_grid = {
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l2'],
    'solver': ['lbfgs']
}
grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5,
                           scoring='accuracy', n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)
best_log_reg = grid_search.best_estimator_
print("Mejores hiperparámetros:", grid_search.best_params_)
print("Mejor precisión en validación cruzada:", grid_search.best_score_)
